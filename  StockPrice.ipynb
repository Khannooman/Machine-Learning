{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ed51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb2b32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Series</th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Trades</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>ADANIENT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>151.90</td>\n",
       "      <td>152.8</td>\n",
       "      <td>157.00</td>\n",
       "      <td>151.05</td>\n",
       "      <td>156.00</td>\n",
       "      <td>156.10</td>\n",
       "      <td>154.50</td>\n",
       "      <td>4377945</td>\n",
       "      <td>6.763819e+13</td>\n",
       "      <td>25254</td>\n",
       "      <td>230979</td>\n",
       "      <td>0.0528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-12-13</td>\n",
       "      <td>ADANIENT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>156.10</td>\n",
       "      <td>156.0</td>\n",
       "      <td>158.60</td>\n",
       "      <td>154.00</td>\n",
       "      <td>154.15</td>\n",
       "      <td>156.00</td>\n",
       "      <td>156.58</td>\n",
       "      <td>3499313</td>\n",
       "      <td>5.479299e+13</td>\n",
       "      <td>20136</td>\n",
       "      <td>241770</td>\n",
       "      <td>0.0691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-14</td>\n",
       "      <td>ADANIENT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>156.00</td>\n",
       "      <td>156.0</td>\n",
       "      <td>157.90</td>\n",
       "      <td>154.10</td>\n",
       "      <td>157.00</td>\n",
       "      <td>157.10</td>\n",
       "      <td>155.90</td>\n",
       "      <td>3389589</td>\n",
       "      <td>5.284529e+13</td>\n",
       "      <td>19917</td>\n",
       "      <td>235044</td>\n",
       "      <td>0.0693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>ADANIENT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>157.10</td>\n",
       "      <td>157.1</td>\n",
       "      <td>158.90</td>\n",
       "      <td>156.35</td>\n",
       "      <td>158.00</td>\n",
       "      <td>157.95</td>\n",
       "      <td>157.60</td>\n",
       "      <td>1595605</td>\n",
       "      <td>2.514683e+13</td>\n",
       "      <td>11049</td>\n",
       "      <td>133437</td>\n",
       "      <td>0.0836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>ADANIENT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>157.95</td>\n",
       "      <td>157.7</td>\n",
       "      <td>159.75</td>\n",
       "      <td>156.20</td>\n",
       "      <td>158.10</td>\n",
       "      <td>158.05</td>\n",
       "      <td>158.14</td>\n",
       "      <td>1689582</td>\n",
       "      <td>2.671824e+13</td>\n",
       "      <td>10488</td>\n",
       "      <td>117265</td>\n",
       "      <td>0.0694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Date    Symbol Series  Prev Close   Open    High     Low  \\\n",
       "0           0  2018-12-12  ADANIENT     EQ      151.90  152.8  157.00  151.05   \n",
       "1           1  2018-12-13  ADANIENT     EQ      156.10  156.0  158.60  154.00   \n",
       "2           2  2018-12-14  ADANIENT     EQ      156.00  156.0  157.90  154.10   \n",
       "3           3  2018-12-17  ADANIENT     EQ      157.10  157.1  158.90  156.35   \n",
       "4           4  2018-12-18  ADANIENT     EQ      157.95  157.7  159.75  156.20   \n",
       "\n",
       "     Last   Close    VWAP   Volume      Turnover  Trades  Deliverable Volume  \\\n",
       "0  156.00  156.10  154.50  4377945  6.763819e+13   25254              230979   \n",
       "1  154.15  156.00  156.58  3499313  5.479299e+13   20136              241770   \n",
       "2  157.00  157.10  155.90  3389589  5.284529e+13   19917              235044   \n",
       "3  158.00  157.95  157.60  1595605  2.514683e+13   11049              133437   \n",
       "4  158.10  158.05  158.14  1689582  2.671824e+13   10488              117265   \n",
       "\n",
       "   %Deliverble  \n",
       "0       0.0528  \n",
       "1       0.0691  \n",
       "2       0.0693  \n",
       "3       0.0836  \n",
       "4       0.0694  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/NOOMAN KHAN/Downloads/stock_nifty50.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9371d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 0\" , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c58bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9368947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.2.0 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)\n",
      "ERROR: No matching distribution found for tensorflow==1.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==1.2.0 --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c966110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Name Of StockWIPRO\n",
      "Number Of Share20\n",
      "Start Date2022-12-16\n",
      "End Date2023-04-16\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 11s 359ms/step - loss: 0.0422 - val_loss: 0.0058\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.0082 - val_loss: 0.0107\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.0024 - val_loss: 0.0148\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.0021 - val_loss: 0.0073\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.0017 - val_loss: 0.0091\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.0017 - val_loss: 0.0040\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.0014 - val_loss: 0.0038\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.0013 - val_loss: 0.0039\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.0013 - val_loss: 0.0037\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.0013 - val_loss: 0.0032\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.0012 - val_loss: 0.0037\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.0011 - val_loss: 0.0019\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.0011 - val_loss: 0.0028\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 9.8485e-04 - val_loss: 0.0020\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 9.5378e-04 - val_loss: 0.0023\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.0010 - val_loss: 0.0018\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 9.1472e-04 - val_loss: 0.0017\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 8.7177e-04 - val_loss: 0.0015\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 8.8454e-04 - val_loss: 0.0017\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 8.4606e-04 - val_loss: 0.0015\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 8.7938e-04 - val_loss: 0.0015\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 8.6385e-04 - val_loss: 0.0014\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 8.7678e-04 - val_loss: 0.0019\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 7.8484e-04 - val_loss: 0.0014\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 7.8702e-04 - val_loss: 0.0014\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 8.3078e-04 - val_loss: 0.0015\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 8.0064e-04 - val_loss: 0.0014\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 8.0190e-04 - val_loss: 0.0012\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 7.4284e-04 - val_loss: 0.0021\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 7.6866e-04 - val_loss: 0.0022\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 7.8276e-04 - val_loss: 0.0013\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 8.7899e-04 - val_loss: 0.0016\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 7.8795e-04 - val_loss: 0.0015\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 7.5901e-04 - val_loss: 0.0013\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 6.9574e-04 - val_loss: 0.0015\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 6.8705e-04 - val_loss: 0.0011\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 7.4071e-04 - val_loss: 0.0011\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 6.8416e-04 - val_loss: 0.0022\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 7.9660e-04 - val_loss: 0.0010\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 8.5876e-04 - val_loss: 9.9774e-04\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 7.5636e-04 - val_loss: 0.0042\n",
      "Profit:  [21567.28498855]\n"
     ]
    }
   ],
   "source": [
    "name_of_stock = input(\"Enter Name Of Stock\")\n",
    "number_of_share = int(input(\"Number Of Share\"))\n",
    "Y = df[df[\"Symbol\"]==name_of_stock][[\"Close\"]]\n",
    "start_date = input(\"Start Date\")\n",
    "end_date = input(\"End Date\")\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler(feature_range=(0,1))\n",
    "Y=scaler.fit_transform(np.array(Y).reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "training_size=int(len(Y)*0.65)\n",
    "test_size=len(Y)-training_size\n",
    "train_data,test_data=Y[0:training_size,:],Y[training_size:len(Y),:1]\n",
    "\n",
    "\n",
    "import numpy\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, time_step):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "\n",
    "time_step = 50\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, ytest = create_dataset(test_data, time_step)\n",
    "\n",
    "\n",
    "X_train =X_train.reshape(X_train.shape[0],X_train.shape[1] , 1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1] , 1)\n",
    "\n",
    "### Create the Stacked LSTM model\n",
    "\n",
    "import tensorflow as tfl\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "model=Sequential()\n",
    "model.add(LSTM(50,return_sequences=True,input_shape=(50,1)))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "model.fit(X_train,y_train,validation_data=(X_test,ytest),epochs=50,batch_size=64,verbose=1)\n",
    "\n",
    "\n",
    "last_date = pd.to_datetime(\"2022-12-12\")\n",
    "lst_output_1=[]\n",
    "lst_output_2=[]\n",
    "\n",
    "diff_1 = start_date-last_date\n",
    "diff_2 = end_date-last_date\n",
    "if int(diff_1.days)>0:\n",
    "    n_steps=50\n",
    "    x_input=test_data[len(test_data)-n_steps:].reshape(1,-1)\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    i=0\n",
    "    while(i<int(diff_1.days)):\n",
    "        if(len(temp_input)>50):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "#                 print(\"{} day input {}\".format(i,x_input))\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "#                 print(\"{} day output {}\".format(i,yhat))\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output_1.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "#                 print(yhat[0])\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "#                 print(len(temp_input))\n",
    "            lst_output_1.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "\n",
    "\n",
    "        \n",
    "if int(diff_2.days)>0:\n",
    "    n_steps=50\n",
    "    x_input=test_data[len(test_data)-n_steps:].reshape(1,-1)\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    i=0\n",
    "    while(i<int(diff_2.days)):\n",
    "        if(len(temp_input)>50):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "#                 print(\"{} day input {}\".format(i,x_input))\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, n_steps, 1))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "#                 print(\"{} day output {}\".format(i,yhat))\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output_2.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, n_steps,1))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "#                 print(yhat[0])\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "#                 print(len(temp_input))\n",
    "            lst_output_2.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        \n",
    "invests = scaler.inverse_transform(lst_output_1)[-1]\n",
    "returns = scaler.inverse_transform(lst_output_2)[-1]\n",
    "Final_Amount = ((returns-invests)*number_of_share)\n",
    "\n",
    "if Final_Amount>0:\n",
    "    print(\"Profit: \" ,Final_Amount)\n",
    "else:\n",
    "    print(\"Loss: \" ,Final_Amount)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
